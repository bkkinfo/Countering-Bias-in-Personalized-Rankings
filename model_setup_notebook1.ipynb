{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "FUkvTIySWBCI"
      },
      "source": [
        "# 1: Designing and evaluating a recommendation algorithm\n",
        "\n",
        "**Hands-on Outline**. In this notebook, we will focus on becoming familiar with the recommendation pipeline through an introductory Python toolbox, in the simplest possible way. Specifically, we will:\n",
        "\n",
        "- **Step 1** Setup the working environment in GDrive. \n",
        "- **Step 2** Load and understand the Movielens 1M dataset.\n",
        "- **Step 3** Split data in training and test sets.\n",
        "- **Step 4** Define a pointwise / pairwise / random / mostpop recommendation algorithm.\n",
        "- **Step 5** Train a recommendation model (only for point-wise and pair-wise).\n",
        "- **Step 6** Compute the user-item matrix that includes the predicted relevance scores.\n",
        "- **Step 7** Calculate evaluation metrics to monitor properties like effectiveness, catalog coverage, and novelty.  \n",
        "- **Step 8** Run the full pipeline for the other algorithms under consideration.   \n",
        "\n",
        "For each step of the pipeline, we will save the corresponding computations (e.g., pre-trained models, user-item relevance matrices and so on). These artifacts will be the starting point of the investigation covered in the subsequent notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWmZsfP5WBCK"
      },
      "source": [
        "## Step 1: Setup the working environment in GDrive. \n",
        "\n",
        "Requirements for your working environment:\n",
        "\n",
        "- Python >= 3.6\n",
        "- Package Requirements: pandas, numpy, scipy, matplotlib, scikit-learn, tensorflow. \n",
        "- GDrive storage requirements: ~1GB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z9U3Pt7WBCL"
      },
      "source": [
        "### Mount the GDrive storage\n",
        "\n",
        "This step serves to mount GDrive storage within this Jupyter notebook. The command will request us to give access permissions to this notebook, so that we will be able to clone the project repository when we desire. Please follow the prompted instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kJDKTL9eWBCL",
        "outputId": "b2f5ba9a-7c9c-407f-9ec2-b1f578cb12da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVFNq50lWBCM"
      },
      "source": [
        "We will clone the project repository in our My Drive folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "buPK6IGfWBCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "993f8bd9-533b-4ad2-e156-eabbfb4ee80c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/My Drive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUGOK-sbWBCM"
      },
      "source": [
        "### Clone the Github repository into GDrive\n",
        "\n",
        "If you want to work with the codebase locally in your laptop, you should start to run the following commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qpZ7OzZ0WBCN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa00d968-d709-49dd-c1c4-b769ed76fe5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'icde2021'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (134/134), done.\u001b[K\n",
            "remote: Compressing objects: 100% (112/112), done.\u001b[K\n",
            "remote: Total 134 (delta 22), reused 130 (delta 18), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (134/134), 11.49 MiB | 9.68 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n",
            "Checking out files: 100% (93/93), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/biasinrecsys/icde2021.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8uUEcyuWBCN"
      },
      "source": [
        "We will move to the project folder in order to install the required packages. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Caq1M0gWWBCN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83848565-5a92-4853-8ee7-32fdb26d3fb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/icde2021\n"
          ]
        }
      ],
      "source": [
        "%cd icde2021"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fIfiX2JoWBCN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a68ce8b-5310-45a4-e844-b5e4ba78117c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  docs  helpers  LICENSE.md  models  notebooks  README.md  requirements.txt\n"
          ]
        }
      ],
      "source": [
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eqja93aXWBCO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6575fff2-ba75-4e8c-c569-66a753ee3591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jupyter\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Collecting jupyterlab\n",
            "  Downloading jupyterlab-3.5.0-py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.7.3)\n",
            "Collecting tensorflow-gpu==2.0\n",
            "  Downloading tensorflow_gpu-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (380.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8 MB 37 kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "  Downloading tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449 kB)\n",
            "\u001b[K     |████████████████████████████████| 449 kB 53.1 MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0->-r requirements.txt (line 8)) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0->-r requirements.txt (line 8)) (0.38.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0->-r requirements.txt (line 8)) (3.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0->-r requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0->-r requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0->-r requirements.txt (line 8)) (1.14.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0->-r requirements.txt (line 8)) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0->-r requirements.txt (line 8)) (1.50.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0->-r requirements.txt (line 8)) (2.1.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "  Downloading tensorboard-2.0.2-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 28.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0->-r requirements.txt (line 8)) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0->-r requirements.txt (line 8)) (3.19.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (3.4.1)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[K     |████████████████████████████████| 152 kB 66.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (0.4.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (0.2.8)\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (4.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (3.2.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 1)) (5.3.4)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 1)) (5.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 1)) (6.1.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 1)) (7.7.1)\n",
            "Collecting qtconsole\n",
            "  Downloading qtconsole-5.4.0-py3-none-any.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 55.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 1)) (5.7.16)\n",
            "Collecting nbclassic\n",
            "  Downloading nbclassic-0.4.8-py3-none-any.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 39.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 2)) (7.9.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 2)) (4.11.2)\n",
            "Requirement already satisfied: jinja2>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 2)) (2.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 2)) (21.3)\n",
            "Collecting jupyter-server<3,>=1.16.0\n",
            "  Downloading jupyter_server-1.23.2-py3-none-any.whl (346 kB)\n",
            "\u001b[K     |████████████████████████████████| 346 kB 65.9 MB/s \n",
            "\u001b[?25hCollecting jupyterlab-server~=2.10\n",
            "  Downloading jupyterlab_server-2.16.3-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting tornado>=6.1.0\n",
            "  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\n",
            "\u001b[K     |████████████████████████████████| 423 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tomli in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 2)) (2.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.1->jupyterlab->-r requirements.txt (line 2)) (2.0.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 2)) (0.13.3)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 2)) (0.15.0)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 2)) (5.1.1)\n",
            "Requirement already satisfied: nbformat>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 2)) (5.7.0)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting nbconvert\n",
            "  Downloading nbconvert-7.2.5-py3-none-any.whl (273 kB)\n",
            "\u001b[K     |████████████████████████████████| 273 kB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.7/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 2)) (6.1.12)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 2)) (23.2.1)\n",
            "Collecting argon2-cffi\n",
            "  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
            "Collecting anyio<4,>=3.1.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 2)) (2.11.0)\n",
            "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 2)) (4.3.3)\n",
            "Collecting json5\n",
            "  Downloading json5-0.9.10-py2.py3-none-any.whl (19 kB)\n",
            "Collecting jinja2>=2.1\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 2)) (5.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 2)) (0.19.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 2)) (22.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (5.0.1)\n",
            "Collecting tinycss2\n",
            "  Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (4.6.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (2.6.1)\n",
            "Collecting mistune<3,>=2.0.3\n",
            "  Downloading mistune-2.0.4-py2.py3-none-any.whl (24 kB)\n",
            "Collecting nbclient>=0.5.0\n",
            "  Downloading nbclient-0.7.0-py3-none-any.whl (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 162 kB/s \n",
            "\u001b[?25hCollecting jupyterlab-pygments\n",
            "  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
            "Collecting nest-asyncio\n",
            "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting traitlets>=5.1\n",
            "  Downloading traitlets-5.5.0-py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 45.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.2.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 2)) (2.16.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->-r requirements.txt (line 1)) (0.2.0)\n",
            "Collecting notebook\n",
            "  Downloading notebook-6.5.2-py3-none-any.whl (439 kB)\n",
            "\u001b[K     |████████████████████████████████| 439 kB 58.3 MB/s \n",
            "\u001b[?25hCollecting notebook-shim>=0.1.0\n",
            "  Downloading notebook_shim-0.2.2-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 5)) (2022.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.2.0)\n",
            "Collecting argon2-cffi-bindings\n",
            "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 2)) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 2)) (2.21)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->-r requirements.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==2.0->-r requirements.txt (line 8)) (1.5.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 2)) (4.4.2)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 57.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 2)) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 2)) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 2)) (2.0.10)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->jupyterlab->-r requirements.txt (line 2)) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->jupyterlab->-r requirements.txt (line 2)) (0.2.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 1)) (3.6.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 1)) (3.0.3)\n",
            "Collecting qtpy>=2.0.1\n",
            "  Downloading QtPy-2.3.0-py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 1.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=008f27798b409a96f38bb306a98065d32a1bff853ef974a4dc34f7cae21ba213\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: traitlets, tornado, nest-asyncio, tinycss2, sniffio, nbclient, mistune, jupyterlab-pygments, jinja2, argon2-cffi-bindings, websocket-client, nbconvert, jedi, argon2-cffi, anyio, jupyter-server, notebook-shim, nbclassic, cachetools, notebook, google-auth, qtpy, json5, tensorflow-estimator, tensorboard, qtconsole, keras-applications, jupyterlab-server, gast, tensorflow-gpu, jupyterlab, jupyter\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.1.1\n",
            "    Uninstalling traitlets-5.1.1:\n",
            "      Successfully uninstalled traitlets-5.1.1\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 6.0.4\n",
            "    Uninstalling tornado-6.0.4:\n",
            "      Successfully uninstalled tornado-6.0.4\n",
            "  Attempting uninstall: mistune\n",
            "    Found existing installation: mistune 0.8.4\n",
            "    Uninstalling mistune-0.8.4:\n",
            "      Successfully uninstalled mistune-0.8.4\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "  Attempting uninstall: nbconvert\n",
            "    Found existing installation: nbconvert 5.6.1\n",
            "    Uninstalling nbconvert-5.6.1:\n",
            "      Successfully uninstalled nbconvert-5.6.1\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.2.0\n",
            "    Uninstalling cachetools-5.2.0:\n",
            "      Successfully uninstalled cachetools-5.2.0\n",
            "  Attempting uninstall: notebook\n",
            "    Found existing installation: notebook 5.7.16\n",
            "    Uninstalling notebook-5.7.16:\n",
            "      Successfully uninstalled notebook-5.7.16\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.14.1\n",
            "    Uninstalling google-auth-2.14.1:\n",
            "      Successfully uninstalled google-auth-2.14.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires tensorboard<2.10,>=2.9, but you have tensorboard 2.0.2 which is incompatible.\n",
            "tensorflow 2.9.2 requires tensorflow-estimator<2.10.0,>=2.9.0rc0, but you have tensorflow-estimator 2.0.1 which is incompatible.\n",
            "tensorflow-probability 0.17.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "google-colab 1.0.0 requires notebook~=5.7.16, but you have notebook 6.5.2 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=6.0.4, but you have tornado 6.2 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\u001b[0m\n",
            "Successfully installed anyio-3.6.2 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 cachetools-4.2.4 gast-0.2.2 google-auth-1.35.0 jedi-0.18.1 jinja2-3.1.2 json5-0.9.10 jupyter-1.0.0 jupyter-server-1.23.2 jupyterlab-3.5.0 jupyterlab-pygments-0.2.2 jupyterlab-server-2.16.3 keras-applications-1.0.8 mistune-2.0.4 nbclassic-0.4.8 nbclient-0.7.0 nbconvert-7.2.5 nest-asyncio-1.5.6 notebook-6.5.2 notebook-shim-0.2.2 qtconsole-5.4.0 qtpy-2.3.0 sniffio-1.3.0 tensorboard-2.0.2 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0 tinycss2-1.2.1 tornado-6.2 traitlets-5.5.0 websocket-client-1.4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCZsWh2AWBCO"
      },
      "source": [
        "We will configure the notebooks directory as our working directory in order to simulate a local notebook execution. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3uOprQ92WBCO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2468c9ba-2dda-4949-9b1e-ceb1b422ddeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/icde2021/notebooks\n"
          ]
        }
      ],
      "source": [
        "%cd ./notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq6eR4DIWBCO"
      },
      "source": [
        "### Import Python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pTUxb1PJWBCO"
      },
      "outputs": [],
      "source": [
        "import sys \n",
        "import os\n",
        "\n",
        "sys.path.append(os.path.join('..'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4x8ErhL0WBCP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yZriPHbOWBCP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gwWrndizWBCP"
      },
      "outputs": [],
      "source": [
        "from helpers.train_test_splitter import *\n",
        "from models.pointwise import PointWise\n",
        "from models.pairwise import PairWise\n",
        "from models.mostpop import MostPop\n",
        "from models.random import Random\n",
        "from helpers.utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me_u_XUqWBCP"
      },
      "source": [
        "###  Create folders for saving pre-computed results\n",
        "\n",
        "We will define the subfolders in **./data** where we will store our pre-computed results. For each dataset:\n",
        "\n",
        "- *data/outputs/splits* will include two csv files including the train and test interactions, according with the selected train-test split rule. \n",
        "- *data/outputs/instances* will include a csv file with instances to be fed to the model, either pairs for point-wise or triplets for pair-wise recommenders.\n",
        "- *data/outputs/models* will include a h5 file associated with a pre-trained recommender model.  \n",
        "- *data/outputs/predictions* will include a numpy file representing a user-item matrix; a cell stores the relevance score of an item for a given user.\n",
        "- *data/outputs/metrics* will include a pickle dictionary with the computed evaluation metrics for a given recommender model. \n",
        "\n",
        "**N.B.** This strategy will allow us to play with the intermediate outputs of the pipeline, without starting from scratch any time (e.g., for performing a bias treatment as a post-processing, we just need to load the predictions of a model to start). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FNXB1l2hWBCP"
      },
      "outputs": [],
      "source": [
        "data_path = '../data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yOKG5uO_WBCQ"
      },
      "outputs": [],
      "source": [
        "!mkdir '../data/outputs'\n",
        "!mkdir '../data/outputs/splits'\n",
        "!mkdir '../data/outputs/instances'\n",
        "!mkdir '../data/outputs/models'\n",
        "!mkdir '../data/outputs/predictions'\n",
        "!mkdir '../data/outputs/metrics'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpFjLalDWBCQ"
      },
      "source": [
        "## Step 2: Load and understand the Movielens 1M dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-vQg4MaWBCQ"
      },
      "source": [
        "First, we will load the **Movielens 1M** dataset, which has been pre-arranged in order to comply with the following structure:\n",
        "\n",
        "- user_id\n",
        "- item_id\n",
        "- rating\n",
        "- timestamp\n",
        "- type (label for the item category\n",
        "- type_id (unique id of the item category)\n",
        "\n",
        "For the sake of tutorial easiness, we assume here that each item is randomly assigned to one of its categories in the original dataset. \n",
        "\n",
        "**N.B.** This toolbox is flexible enough to integrate any other dataset in csv format that has the same structure of the pre-arranged csv shown below. No further changes are then needed to the pipeline in order to experiment with other datasets. The csv file of the new dataset should be placed into the *data/datasets/* folder and the name of the file should be assigned to the *dataset* parameter below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pOZfWBzlWBCQ"
      },
      "outputs": [],
      "source": [
        "dataset = 'ml1m'  \n",
        "user_field = 'user_id'\n",
        "item_field = 'item_id'\n",
        "rating_field = 'rating'\n",
        "time_field = 'timestamp'\n",
        "type_field = 'type_id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xmq2bCkrWBCQ"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(os.path.join(data_path, 'datasets/' + dataset + '.csv'), encoding='utf8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "U-c4RKn1WBCQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "91fa19f6-0a3e-4c0f-e82f-2b6c766e17ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        user_id  item_id  rating            timestamp       type  type_id\n",
              "630120     3752      319     4.0  2000-08-13 01:03:20   Thriller       15\n",
              "229398     2411       39     4.0  2000-11-19 00:38:54    Romance       13\n",
              "758377     2172     2915     3.0  2002-03-02 22:18:54     Comedy        4\n",
              "159240     2366      349     3.0  2000-11-16 03:11:21     Action        0\n",
              "254252     1017      377     4.0  2000-11-23 21:15:04   Thriller       15\n",
              "27168      3391      527     5.0  2000-08-29 19:18:26      Drama        7\n",
              "196538     2376     1580     4.0  2002-06-19 12:58:43     Comedy        4\n",
              "37123      4635      783     3.0  2000-07-19 21:31:53    Musical       11\n",
              "982048      868     1346     4.0  2000-11-26 23:13:31     Horror       10\n",
              "994502     1880     2275     4.0  2000-11-20 05:37:43  Adventure        1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d85e1e3d-ff04-4e82-9547-04828cb0b590\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>type</th>\n",
              "      <th>type_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>630120</th>\n",
              "      <td>3752</td>\n",
              "      <td>319</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2000-08-13 01:03:20</td>\n",
              "      <td>Thriller</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229398</th>\n",
              "      <td>2411</td>\n",
              "      <td>39</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2000-11-19 00:38:54</td>\n",
              "      <td>Romance</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>758377</th>\n",
              "      <td>2172</td>\n",
              "      <td>2915</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2002-03-02 22:18:54</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159240</th>\n",
              "      <td>2366</td>\n",
              "      <td>349</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2000-11-16 03:11:21</td>\n",
              "      <td>Action</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254252</th>\n",
              "      <td>1017</td>\n",
              "      <td>377</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2000-11-23 21:15:04</td>\n",
              "      <td>Thriller</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27168</th>\n",
              "      <td>3391</td>\n",
              "      <td>527</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2000-08-29 19:18:26</td>\n",
              "      <td>Drama</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196538</th>\n",
              "      <td>2376</td>\n",
              "      <td>1580</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2002-06-19 12:58:43</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37123</th>\n",
              "      <td>4635</td>\n",
              "      <td>783</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2000-07-19 21:31:53</td>\n",
              "      <td>Musical</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982048</th>\n",
              "      <td>868</td>\n",
              "      <td>1346</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2000-11-26 23:13:31</td>\n",
              "      <td>Horror</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994502</th>\n",
              "      <td>1880</td>\n",
              "      <td>2275</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2000-11-20 05:37:43</td>\n",
              "      <td>Adventure</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d85e1e3d-ff04-4e82-9547-04828cb0b590')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d85e1e3d-ff04-4e82-9547-04828cb0b590 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d85e1e3d-ff04-4e82-9547-04828cb0b590');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "data.sample(n=10, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzGSQNDkWBCR"
      },
      "source": [
        "### Short exercise 1: find the id of the most popular item (i.e, the item with the highest number of ratings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "M67lP58GWBCR"
      },
      "outputs": [],
      "source": [
        "### EXERCISE CELL ### Please, add your solution here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-z-URfpWBCR"
      },
      "source": [
        "During this tutorial, we will simulate a scenario with **implicit feedback**. We assume that a user is interested in an item, if that item was rated by the user, no matter of the rating value. Other strategies can be easily integrated. \n",
        "\n",
        "**N.B.** Other papers in the literature assumed that an item is relevant for a user, only if the user has given a rating higher than a value X. To implement this strategy here, you just need to change the body of the lambda function below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "asA9GYF6WBCR"
      },
      "outputs": [],
      "source": [
        "data[rating_field] = data[rating_field].apply(lambda x: 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Rrw3OOiPWBCR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "0ee8e947-1487-4348-cb6b-4fa7b2dcdf0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        user_id  item_id  rating            timestamp       type  type_id\n",
              "630120     3752      319     1.0  2000-08-13 01:03:20   Thriller       15\n",
              "229398     2411       39     1.0  2000-11-19 00:38:54    Romance       13\n",
              "758377     2172     2915     1.0  2002-03-02 22:18:54     Comedy        4\n",
              "159240     2366      349     1.0  2000-11-16 03:11:21     Action        0\n",
              "254252     1017      377     1.0  2000-11-23 21:15:04   Thriller       15\n",
              "27168      3391      527     1.0  2000-08-29 19:18:26      Drama        7\n",
              "196538     2376     1580     1.0  2002-06-19 12:58:43     Comedy        4\n",
              "37123      4635      783     1.0  2000-07-19 21:31:53    Musical       11\n",
              "982048      868     1346     1.0  2000-11-26 23:13:31     Horror       10\n",
              "994502     1880     2275     1.0  2000-11-20 05:37:43  Adventure        1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0a9df45d-7beb-44ab-aa39-dcab0ca13109\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>type</th>\n",
              "      <th>type_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>630120</th>\n",
              "      <td>3752</td>\n",
              "      <td>319</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2000-08-13 01:03:20</td>\n",
              "      <td>Thriller</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229398</th>\n",
              "      <td>2411</td>\n",
              "      <td>39</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2000-11-19 00:38:54</td>\n",
              "      <td>Romance</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>758377</th>\n",
              "      <td>2172</td>\n",
              "      <td>2915</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2002-03-02 22:18:54</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159240</th>\n",
              "      <td>2366</td>\n",
              "      <td>349</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2000-11-16 03:11:21</td>\n",
              "      <td>Action</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254252</th>\n",
              "      <td>1017</td>\n",
              "      <td>377</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2000-11-23 21:15:04</td>\n",
              "      <td>Thriller</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27168</th>\n",
              "      <td>3391</td>\n",
              "      <td>527</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2000-08-29 19:18:26</td>\n",
              "      <td>Drama</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196538</th>\n",
              "      <td>2376</td>\n",
              "      <td>1580</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2002-06-19 12:58:43</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37123</th>\n",
              "      <td>4635</td>\n",
              "      <td>783</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2000-07-19 21:31:53</td>\n",
              "      <td>Musical</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982048</th>\n",
              "      <td>868</td>\n",
              "      <td>1346</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2000-11-26 23:13:31</td>\n",
              "      <td>Horror</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994502</th>\n",
              "      <td>1880</td>\n",
              "      <td>2275</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2000-11-20 05:37:43</td>\n",
              "      <td>Adventure</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a9df45d-7beb-44ab-aa39-dcab0ca13109')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0a9df45d-7beb-44ab-aa39-dcab0ca13109 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0a9df45d-7beb-44ab-aa39-dcab0ca13109');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "data.sample(n=10, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVVwtZgdWBCR"
      },
      "source": [
        "## Step 3: Split data in training and test sets\n",
        "\n",
        "Once the original dataset has been loaded and the user preferences have been pre-processed, we need to split the whole dataset in two sets: a training set used for optimizing the recommender model and a test set used for evaluating the recommender model. In the literature, a wide range of train-test split strategy exists. This notebook will use a strategy that, for each user, puts the oldest interactions in the training set and the most recent interactions in the test set. The Python toolbox includes also other strategies, such as a random split or a split based on a fixed timestamp (i.e., the most realistic one).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uqHVF5xWBCR"
      },
      "source": [
        "- **smode**: 'uftime' for fixed timestamp split, 'utime' for time-based split per user, 'urandom' for random split per user \n",
        "- **train_ratio**: percentage of data to be included in the train set\n",
        "- **min_train**: minimum number of train samples for a user to be included  \n",
        "- **min_test**: minimum number of test samples for a user to be included\n",
        "- **min_time**: start timestamp for computing the splitting timestamp (only for uftime)\n",
        "- **max_time**: end timestamp for computing the splitting timestamp (only for uftime)\n",
        "- **step_time**: timestamp step for computing the splitting timestamp (only for uftime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "viWWbZZkWBCR"
      },
      "outputs": [],
      "source": [
        "smode = 'utime'\n",
        "train_ratio = 0.80        \n",
        "min_train_samples = 8\n",
        "min_test_samples = 2\n",
        "min_time = None\n",
        "max_time = None\n",
        "step_time = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzqKJDusWBCS"
      },
      "source": [
        "During this tutorial, we will work with a common **time-based split per user**. For the sake of clarity, we will provide the implementation of this strategy below. The toolbox conserves all the train-test split strategies into the file *helpers/train_test_splitter.py*.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "sVamu5b1WBCS"
      },
      "outputs": [],
      "source": [
        "def user_timestamp(interactions,split=0.80,min_samples=10,user_field='user_id',item_field='item_id',time_field='timestamp'):\n",
        "    train_set = []\n",
        "    test_set = []\n",
        "    \n",
        "    groups = interactions.groupby([user_field])\n",
        "    for i, (index, group) in enumerate(groups):\n",
        "        \n",
        "        if len(group.index) < min_samples:\n",
        "            continue\n",
        "        \n",
        "        sorted_group = group.sort_values(time_field)\n",
        "        n_rating_test = int(len(sorted_group.index) * (1.0 - split))\n",
        "        train_set.append(sorted_group.head(len(sorted_group.index) - n_rating_test))\n",
        "        test_set.append(sorted_group.tail(n_rating_test))\n",
        "    \n",
        "    print('\\r> Parsing user', i+1, 'of', len(groups))\n",
        "\n",
        "    train, test = pd.concat(train_set), pd.concat(test_set)\n",
        "    train['set'], test['set'] = 'train', 'test' # Ensure that each row has a column that identifies the associated set\n",
        "\n",
        "    traintest = pd.concat([train, test])\n",
        "    traintest[user_field + '_original'] = traintest[user_field] # Ensure that we save the original user ids\n",
        "    traintest[item_field + '_original'] = traintest[item_field] # Ensure that we save the original item ids\n",
        "    traintest[user_field] = traintest[user_field].astype('category').cat.codes # Ensure that user ids are in [0, |U|] \n",
        "    traintest[item_field] = traintest[item_field].astype('category').cat.codes # Ensure that item ids are in [0, |I|] \n",
        "\n",
        "    return traintest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrq7p97AWBCS"
      },
      "source": [
        "### Perform the training and test set split\n",
        "\n",
        "This notebook can be easily run with any of the different train-test split strategies, through the following code. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YGvVeCZVWBCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b141287-8398-40ab-bd09-a9e57bae8322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r> Parsing user 6040 of 6040\n"
          ]
        }
      ],
      "source": [
        "if smode == 'uftime':\n",
        "    traintest = fixed_timestamp(data, min_train_samples, min_test_samples, min_time, max_time, step_time, user_field, item_field, time_field, rating_field)\n",
        "elif smode == 'utime':\n",
        "    traintest = user_timestamp(data, train_ratio, min_train_samples+min_test_samples, user_field, item_field, time_field)\n",
        "elif smode == 'urandom':\n",
        "    traintest = user_random(data, train_ratio, min_train_samples+min_test_samples, user_field, item_field)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4YyhI20WBCU"
      },
      "source": [
        "**N.B.** For the sake of convenience, *user_ids* and *item_ids* have been scaled so that user_ids are in *[0, |U|]* and item_ids are in *[0, |I|]*. To refer back to the original user and item ids, the *user_id_original* and *item_id_original* columns should be used. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCXiOIHtWBCU"
      },
      "source": [
        "For the sake of replicability and efficiency of this tutorial, we will save the pre-computed train and test sets in *data/outputs/splits*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WQ3GoNWgWBCU"
      },
      "outputs": [],
      "source": [
        "traintest.to_csv(os.path.join(data_path, 'outputs/splits/' + dataset + '_' + smode + '.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tuRD_fYJWBCU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "9508b1ec-2c57-4c37-b558-6236c1a3117c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        user_id  item_id  rating            timestamp       type  type_id  \\\n",
              "461252     4681     1980     1.0  2000-08-06 02:31:13  Adventure        1   \n",
              "432180     1700     2303     1.0  2001-12-30 08:15:18    Romance       13   \n",
              "396031     5688      759     1.0  2000-06-26 05:01:36      Drama        7   \n",
              "692829     1217     1611     1.0  2000-12-01 18:23:13     Comedy        4   \n",
              "333979     1882     2597     1.0  2000-11-22 07:57:02     Action        0   \n",
              "524446      226     2958     1.0  2000-12-14 22:41:09  Adventure        1   \n",
              "52926      1472     1848     1.0  2000-11-20 21:38:38     Action        0   \n",
              "895248      307      313     1.0  2001-07-22 22:58:50      Drama        7   \n",
              "285473     5491     2748     1.0  2001-08-07 19:22:33      Drama        7   \n",
              "94374      5851     3031     1.0  2000-05-12 05:35:27     Comedy        4   \n",
              "\n",
              "          set  user_id_original  item_id_original  \n",
              "461252  train              4682              2161  \n",
              "432180  train              1701              2497  \n",
              "396031  train              5689               805  \n",
              "692829  train              1218              1772  \n",
              "333979  train              1883              2802  \n",
              "524446  train               227              3175  \n",
              "52926   train              1473              2028  \n",
              "895248  train               308               322  \n",
              "285473   test              5492              2959  \n",
              "94374    test              5852              3255  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3dc18085-558d-4e3c-a616-4fb706eb6fef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>type</th>\n",
              "      <th>type_id</th>\n",
              "      <th>set</th>\n",
              "      <th>user_id_original</th>\n",
              "      <th>item_id_original</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>461252</th>\n",
              "      <td>4681</td>\n",
              "      <td>1980</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2000-08-06 02:31:13</td>\n",
              "      <td>Adventure</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td>4682</td>\n",
              "      <td>2161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>432180</th>\n",
              "      <td>1700</td>\n",
              "      <td>2303</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2001-12-30 08:15:18</td>\n",
              "      <td>Romance</td>\n",
              "      <td>13</td>\n",
              "      <td>train</td>\n",
              "      <td>1701</td>\n",
              "      <td>2497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396031</th>\n",
              "      <td>5688</td>\n",
              "      <td>759</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2000-06-26 05:01:36</td>\n",
              "      <td>Drama</td>\n",
              "      <td>7</td>\n",
              "      <td>train</td>\n",
              "      <td>5689</td>\n",
              "      <td>805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>692829</th>\n",
              "      <td>1217</td>\n",
              "      <td>1611</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2000-12-01 18:23:13</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>4</td>\n",
              "      <td>train</td>\n",
              "      <td>1218</td>\n",
              "      <td>1772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333979</th>\n",
              "      <td>1882</td>\n",
              "      <td>2597</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2000-11-22 07:57:02</td>\n",
              "      <td>Action</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>1883</td>\n",
              "      <td>2802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524446</th>\n",
              "      <td>226</td>\n",
              "      <td>2958</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2000-12-14 22:41:09</td>\n",
              "      <td>Adventure</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td>227</td>\n",
              "      <td>3175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52926</th>\n",
              "      <td>1472</td>\n",
              "      <td>1848</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2000-11-20 21:38:38</td>\n",
              "      <td>Action</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>1473</td>\n",
              "      <td>2028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>895248</th>\n",
              "      <td>307</td>\n",
              "      <td>313</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2001-07-22 22:58:50</td>\n",
              "      <td>Drama</td>\n",
              "      <td>7</td>\n",
              "      <td>train</td>\n",
              "      <td>308</td>\n",
              "      <td>322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285473</th>\n",
              "      <td>5491</td>\n",
              "      <td>2748</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2001-08-07 19:22:33</td>\n",
              "      <td>Drama</td>\n",
              "      <td>7</td>\n",
              "      <td>test</td>\n",
              "      <td>5492</td>\n",
              "      <td>2959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94374</th>\n",
              "      <td>5851</td>\n",
              "      <td>3031</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2000-05-12 05:35:27</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>4</td>\n",
              "      <td>test</td>\n",
              "      <td>5852</td>\n",
              "      <td>3255</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3dc18085-558d-4e3c-a616-4fb706eb6fef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3dc18085-558d-4e3c-a616-4fb706eb6fef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3dc18085-558d-4e3c-a616-4fb706eb6fef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "traintest.sample(n=10, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeEK-4MAWBCU"
      },
      "source": [
        "## Step 4: Define a pointwise / pairwise / random / mostpop recommendation algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "BfMHn7rIWBCU"
      },
      "outputs": [],
      "source": [
        "train = traintest[traintest['set']=='train'].copy()\n",
        "test = traintest[traintest['set']=='test'].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvJcQlh2WBCV"
      },
      "source": [
        "### Short exercise 2: plot the distribution of interactions per item in the training set and in the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "eb-zXZfXWBCV"
      },
      "outputs": [],
      "source": [
        "### EXERCISE CELL ### Please, add your solution here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5zB7BDvWBCV"
      },
      "source": [
        "First, we show some statistics about the training and test sets, e.g., number of users and items. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Aucc5B-xWBCV"
      },
      "outputs": [],
      "source": [
        "users = list(np.unique(traintest[user_field].values))\n",
        "items = list(np.unique(traintest[item_field].values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "APDDTBlAWBCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d683b30-e899-4860-fb69-1bb85e7d51a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6040, 3706)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "len(users), len(items)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf8bRShvWBCV"
      },
      "source": [
        "Given that some recommender models may require the category of an item, we create a vector of size *|I|* including the integer-encoded category of the item with id *X* at position *X* of the vector. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "OxM4t37NWBCV"
      },
      "outputs": [],
      "source": [
        "category_per_item = traintest.drop_duplicates(subset=['item_id'], keep='first')[type_field].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "MsZnl2uPWBCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6652017-f277-44ae-dd92-d90c81503a6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "len(np.unique(category_per_item))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwPYFViOWBCW"
      },
      "source": [
        "### Initialize the recommendation algorithm object\n",
        "\n",
        "For the sake of easiness and time, this tutorial focuses on four main recommendation strategies: \n",
        "\n",
        "**Random**: randomly recommending a list of items to a user. \n",
        "\n",
        "**MostPop**: recommending the same most popular items (i.e, those which received the highest number of ratings) to all users.\n",
        "\n",
        "**PointWise**: given a user-item pair, it is optimized for predicting a higher score (1) when the current item has been rated by the user, and a lower score (0) otherwise. The training instances include a good reprsentation of both types of pairs.   \n",
        "\n",
        "**PairWise**: given a triplet with a user, an observed item, and an unobserved item, it is optimized for predicting a higher relevance for the pair of user and unobserved item rather than for the pair of user and unobserved item. \n",
        "\n",
        "Each model inherits from the Model class defined in *models/model.py* and extends it by overwriting the *train* and *predict* functions of the original model class. This allows us to minimize the reuse of the code. More details on the implementation of the pairwise recommender can be found into *models/pairwise.py*.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "YOqjbZ04WBCW"
      },
      "outputs": [],
      "source": [
        "model_types = {'random': Random, 'mostpop': MostPop, 'pointwise': PointWise, 'pairwise': PairWise}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt2qoek4WBCW"
      },
      "source": [
        "First, we need to initialize the model. We will see how the process works for a PairWise algorithm. Then, we will consider the other ones. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "foY5GAE8WBCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29f74834-7fa5-4049-aebd-960d3e0dd5d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing user, item, and categories lists\n",
            "Initializing observed, unobserved, and predicted relevance scores\n",
            "Initializing item popularity lists\n",
            "Initializing category per item\n",
            "Initializing category preference per user\n",
            "Initializing metrics\n",
            "CPU times: user 5.2 s, sys: 198 ms, total: 5.39 s\n",
            "Wall time: 5.41 s\n"
          ]
        }
      ],
      "source": [
        "model_type = 'pairwise'\n",
        "%time model = PairWise(users, items, train, test, category_per_item, item_field, user_field, rating_field)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY_hCx0IWBCW"
      },
      "source": [
        "## Step 5: Train a recommendation model (only for point-wise and pair-wise).\n",
        "\n",
        "We will train the model by feeding the train data we previously prepared, using the following default parameters. \n",
        "\n",
        "- **no_epochs** (default 100): maximum number of epochs until which the training process will be run. \n",
        "- **batches** (default 1024): size of the batches fed into the model during training. \n",
        "- **lr** (default 0.001): learning rate defining the pace at which the model will be trained. \n",
        "- **no_factors** (default 10): size of the latent vectors associated to users and items. \n",
        "- **no_negatives** (default 10): number of triplets for each user-item pair included in the training set. \n",
        "- **val_split** (default 0.0001): proportion of the training set used for validation. \n",
        "\n",
        "**N.B.** For the sake of tutorial efficiency, we force to stop the training process after 5 epochs (i.e., reasonable trade-off). No grid search on the recommender model is performed at this stage. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "EYlDH7efWBCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "670094e6-199d-4bdb-b4cf-8697c4333da6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating training instances of type pair\n",
            "Computing instances for interaction 800000 / 803798 of type pair\n",
            "Performing training - Epochs 5 Batch Size 1024 Learning Rate 0.001 Factors 10 Negatives 10 Mode pair\n",
            "Train on 7957600 samples\n",
            "7957600/7957600 [==============================] - 39s 5us/sample - loss: 0.1914\n",
            "Validation accuracy: 0.8622916148295596 (Sample 80379 of 80380)\n",
            "Train on 7957600 samples\n",
            "Epoch 2/2\n",
            "7957600/7957600 [==============================] - 38s 5us/sample - loss: 0.1313\n",
            "Train on 7957600 samples\n",
            "Epoch 3/3\n",
            "7957600/7957600 [==============================] - 42s 5us/sample - loss: 0.1094\n",
            "Train on 7957600 samples\n",
            "Epoch 4/4\n",
            "7957600/7957600 [==============================] - 39s 5us/sample - loss: 0.0966\n",
            "Train on 7957600 samples\n",
            "Epoch 5/5\n",
            "7957600/7957600 [==============================] - 38s 5us/sample - loss: 0.0886\n",
            "Validation accuracy: 0.9164966409554616 (Sample 80379 of 80380)\n",
            "CPU times: user 5min 49s, sys: 11.9 s, total: 6min 1s\n",
            "Wall time: 4min 46s\n"
          ]
        }
      ],
      "source": [
        "%time model.train(no_epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JF7548JWBCW"
      },
      "source": [
        "The architecture of the trained model looks as follows. Essentially, the model includes:\n",
        "- **UserEmb** encoding a latent vector for each user.\n",
        "- **ItemEmb** encoding a latent vector for each item.\n",
        "- **FlatUserEmb** represents the vector associated with the current user *UserInput*.\n",
        "- **FlatPosItemEmb** represents the vectors associated with the current observed item *PosItemInput*.\n",
        "- **FlatNegItemEmb** represents the vectors associated with the current unobserved item *NegItemInput*.\n",
        "- **Accuracy** computes the margin between (i) the *FlatUserEmb-FlatPosItemEmb* and (ii) the *FlatUserEmb-FlatNegItemEmb* similarity scores.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "9JuGmqqZWBCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d7331d-2e0b-413c-c0cf-6c3d763f348e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "UserInput (InputLayer)          [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "PosItemInput (InputLayer)       [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "NegItemInput (InputLayer)       [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "UserEmb (Embedding)             (None, 1, 10)        60410       UserInput[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "ItemEmb (Embedding)             (None, 1, 10)        37070       PosItemInput[0][0]               \n",
            "                                                                 NegItemInput[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "FlatUserEmb (Flatten)           (None, 10)           0           UserEmb[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "FlatPosItemEmb (Flatten)        (None, 10)           0           ItemEmb[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "FlatNegItemEmb (Flatten)        (None, 10)           0           ItemEmb[1][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "Accuracy (Lambda)               (None, 1)            0           FlatUserEmb[0][0]                \n",
            "                                                                 FlatPosItemEmb[0][0]             \n",
            "                                                                 FlatNegItemEmb[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 97,480\n",
            "Trainable params: 97,480\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkI0VyxWWBCW"
      },
      "source": [
        "The model file is saved in *data/outputs/models*. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "psivSTHZWBCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbcdcd41-109e-4957-93e7-d2381c817165"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<models.pairwise.PairWise at 0x7f821300ec10>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPOVsirFWBCX"
      },
      "source": [
        "## Step 6: Compute the user-item matrix that includes the predicted relevance scores.\n",
        "\n",
        "Once the recommender model has been trained, we leverage the pre-trained user and item Embedding matrices in order to compute the relevance score predicted for each unseen user-item pair. For all the user-item pairs, the prediction step requires to extract the user and item vector associated to the current user-item pair and, then, compute the similarity between the two - cosine or dot similarity are usually used at this stage.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "AWk4Hl62WBCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e4a9987-de84-46ba-e44f-976b3101abc7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<models.pairwise.PairWise at 0x7f821300ec10>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y655-wWaWBCX"
      },
      "source": [
        "Now, we will use the pre-trained model to predict the user-item relevance scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "yD14FreaWBCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "311ca796-71e3-4891-a9b2-7b311a5febe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing predictions\n"
          ]
        }
      ],
      "source": [
        "model.predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVMq3tFYWBCX"
      },
      "source": [
        "For the sake of easiness, you could directly manipulate the user-item relevance matrix as a numpy array. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ahgao7vHWBCX"
      },
      "outputs": [],
      "source": [
        "scores = model.get_predictions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9_W1wqxWBCX"
      },
      "source": [
        "Hence, we can access to the relevance score of the user *120* for the item *320* as follows. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "EceVHzVKWBCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "467b815c-0a11-4ba7-c6fb-f6019eb1574e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.069070816040039"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "user_id, item_id = 120, 320\n",
        "scores[user_id, item_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoZ-hWh0WBCX"
      },
      "source": [
        "### Short exercise 3: compute the range of the scores on the whole population of users.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqi4ZiSwWBCY"
      },
      "source": [
        "For the sake of convenience, we will save the predicted scores. They are often used as an input for re-ranking treatments against bias. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "F11J10-4WBCY"
      },
      "outputs": [],
      "source": [
        "save_obj(scores, os.path.join(data_path, 'outputs/predictions/' + dataset + '_' + smode + '_' + model_type + '_scores.pkl'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQe65kZHWBCY"
      },
      "source": [
        "### Short exercise 4: retrieve the ids of the 10 items with  the highest relevance score for user 47.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "dpIiHWUIWBCY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dcdca09-1a80-444f-acb0-3a8fd23728c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6040, 3706)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "scores.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWQalZhgWBCY"
      },
      "source": [
        "## Step 7: Calculate evaluation metrics.\n",
        "\n",
        "Finally, with the user-item relevance scores predicted in the previous step, we can generate the recommendations for each user and, then, compute a set of well-known evaluation metrics for recommender systems. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "bffuze7QWBCY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea8b97f-d578-44cf-f326-8cf5aa1ab67a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6040, 3706)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "scores.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "AwkVoheMWBCY"
      },
      "outputs": [],
      "source": [
        "cutoffs = np.array([5, 10, 20, 50, 100, 200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HLmWKd2WBCY"
      },
      "source": [
        "Then, we run the function which computes all the metrics relevant for the subsequent case studies. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Q7rEowV6WBCY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0df3ac44-1fd0-4b27-f608-76933d84f63f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing metrics for user 6040 / 6040\n"
          ]
        }
      ],
      "source": [
        "model.test(cutoffs=cutoffs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI_HXwrZWBCZ"
      },
      "source": [
        "The method has pre-computed a set of metrics and saved the corresponding values in a Python dictionary, as detailed below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "jz88Sk5hWBCZ"
      },
      "outputs": [],
      "source": [
        "metrics = model.get_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "yc8VNJskWBCZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b802cdfd-07c5-4140-e192-87233c59efbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['precision', 'recall', 'ndcg', 'hit', 'mean_popularity', 'diversity', 'novelty', 'item_coverage'])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "metrics.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twhUT1SlWBCZ"
      },
      "source": [
        "The values for each metrics have been computed and store for each cutoff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "RcY9IR_RWBCZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4604100-c41f-4c5b-98c7-6913d370fca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 6040) precision\n",
            "(6, 6040) recall\n",
            "(6, 6040) ndcg\n",
            "(6, 6040) hit\n",
            "(6, 6040) mean_popularity\n",
            "(6, 6040) diversity\n",
            "(6, 6040) novelty\n",
            "(6, 3706) item_coverage\n"
          ]
        }
      ],
      "source": [
        "for name, values in metrics.items():\n",
        "    print(values.shape, name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZBnSpkqWBCZ"
      },
      "source": [
        "For instance, we can access to the NDCG score for the user *120* at cutoff *10*, with the following commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "AdwpUG5GWBCZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c110c5f8-d5bd-4414-dcf6-bbc5fc0b3858"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.37566986542073244"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "user_id, cutoff_index = 1324, int(np.where(cutoffs == 10)[0])\n",
        "metrics['ndcg'][cutoff_index, user_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfH5AqXLWBCZ"
      },
      "source": [
        "### Short exercise 6: compute catalog coverage (i.e., percentage of items recommended at least once) at top-20.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWBWZf8iWBCZ"
      },
      "source": [
        "For the sake of convenience, we will save the compted metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "44TTRP4DWBCZ"
      },
      "outputs": [],
      "source": [
        "save_obj(metrics, os.path.join(data_path, 'outputs/metrics/' + dataset + '_' + smode + '_' + model_type + '_metrics.pkl'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuFsWeqWWBCa"
      },
      "source": [
        "We can also see the aggregated values. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "IorhIShTWBCa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfbeeda8-5893-47cd-f49a-f3b01a00ce5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.1167 \n",
            "Recall: 0.0485 \n",
            "NDCG: 0.1266 \n",
            "Hit Rate: 0.5197 \n",
            "Avg Popularity: 1955.2443 \n",
            "Category Diversity: 0.3308 \n",
            "Novelty: 1.7483 \n",
            "Item Coverage: 0.21 \n",
            "User Coverage: 0.5197\n"
          ]
        }
      ],
      "source": [
        "model.show_metrics(index_k=int(np.where(cutoffs == 10)[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "YnW5jbN9WBCa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "67c14667-6e48-407a-f405-3808fc838253"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'precision - recall - ndcg - hit - mean_popularity - diversity - novelty - item_coverage'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "' - '.join(list(metrics.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6Hu3PuBWBCa"
      },
      "source": [
        "## Step 8: Run the full pipeline for the other algorithms under consideration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2taFzErWBCa"
      },
      "source": [
        "We will define a utility function to run all the above operations jointly for each of the other recommender models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "rXjnzcCRWBCa"
      },
      "outputs": [],
      "source": [
        "def run_model(model_type, no_epochs=None):\n",
        "    print('Running model', model_type)\n",
        "    # Initialize the model\n",
        "    model = model_types[model_type](users, items, train, test, category_per_item, item_field, user_field, rating_field)\n",
        "    # Train the model\n",
        "    model.train(no_epochs=no_epochs) if no_epochs else model.train() \n",
        "    # Make and save predictions\n",
        "    model.predict()\n",
        "    scores = model.get_predictions()\n",
        "    save_obj(scores, os.path.join(data_path, 'outputs/predictions/' + dataset + '_' + smode + '_' + model_type + '_scores.pkl'))\n",
        "    # Compute and save metrics\n",
        "    model.test(cutoffs=cutoffs)\n",
        "    metrics = model.get_metrics()\n",
        "    save_obj(metrics, os.path.join(data_path, 'outputs/metrics/' + dataset + '_' + smode + '_' + model_type + '_metrics.pkl'))\n",
        "    # Show evaluation metrics\n",
        "    print('\\n\\nFinal evaluation metrics:')\n",
        "    model.show_metrics(index_k=int(np.where(cutoffs == 10)[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "-pZcF0tgWBCa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a308d75-bd92-4f8c-b305-99cd204349ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running model random\n",
            "Initializing user, item, and categories lists\n",
            "Initializing observed, unobserved, and predicted relevance scores\n",
            "Initializing item popularity lists\n",
            "Initializing category per item\n",
            "Initializing category preference per user\n",
            "Initializing metrics\n",
            "Computing predictions\n",
            "Computing metrics for user 6040 / 6040\n",
            "\n",
            "\n",
            "Final evaluation metrics:\n",
            "Precision: 0.0095 \n",
            "Recall: 0.0028 \n",
            "NDCG: 0.0095 \n",
            "Hit Rate: 0.0844 \n",
            "Avg Popularity: 198.9814 \n",
            "Category Diversity: 0.328 \n",
            "Novelty: 6.9589 \n",
            "Item Coverage: 1.0 \n",
            "User Coverage: 0.0844\n"
          ]
        }
      ],
      "source": [
        "run_model('random')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "D5huqa92WBCa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0803894-5fac-44d8-909d-c101bb2c7af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running model mostpop\n",
            "Initializing user, item, and categories lists\n",
            "Initializing observed, unobserved, and predicted relevance scores\n",
            "Initializing item popularity lists\n",
            "Initializing category per item\n",
            "Initializing category preference per user\n",
            "Initializing metrics\n",
            "Computing predictions\n",
            "Computing metrics for user 6040 / 6040\n",
            "\n",
            "\n",
            "Final evaluation metrics:\n",
            "Precision: 0.1007 \n",
            "Recall: 0.0384 \n",
            "NDCG: 0.1096 \n",
            "Hit Rate: 0.4422 \n",
            "Avg Popularity: 2328.0848 \n",
            "Category Diversity: 0.3293 \n",
            "Novelty: 1.3922 \n",
            "Item Coverage: 0.03 \n",
            "User Coverage: 0.4422\n"
          ]
        }
      ],
      "source": [
        "run_model('mostpop')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ehfYRFtMWBCa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5150d0b-11b6-4987-9b15-401840eb7d3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running model pointwise\n",
            "Initializing user, item, and categories lists\n",
            "Initializing observed, unobserved, and predicted relevance scores\n",
            "Initializing item popularity lists\n",
            "Initializing category per item\n",
            "Initializing category preference per user\n",
            "Initializing metrics\n",
            "Generating training instances of type point\n",
            "Computing instances for interaction 800000 / 803798 of type point\n",
            "Performing training - Epochs 5 Batch Size 1024 Learning Rate 0.001 Factors 10 Negatives 10 Mode point\n",
            "Train on 7957600 samples, validate on 884178 samples\n",
            "Epoch 1/5\n",
            "7957600/7957600 [==============================] - 96s 12us/sample - loss: 0.2093 - val_loss: 0.2338\n",
            "Epoch 2/5\n",
            "7957600/7957600 [==============================] - 94s 12us/sample - loss: 0.1752 - val_loss: 0.2389\n",
            "Epoch 3/5\n",
            "7957600/7957600 [==============================] - 97s 12us/sample - loss: 0.1670 - val_loss: 0.2408\n",
            "Epoch 00003: early stopping\n",
            "Computing metrics for user 6040 / 6040\n",
            "\n",
            "\n",
            "Final evaluation metrics:\n",
            "Precision: 0.1184 \n",
            "Recall: 0.0592 \n",
            "NDCG: 0.1306 \n",
            "Hit Rate: 0.5608 \n",
            "Avg Popularity: 1445.2056 \n",
            "Category Diversity: 0.3247 \n",
            "Novelty: 2.3052 \n",
            "Item Coverage: 0.33 \n",
            "User Coverage: 0.5608\n"
          ]
        }
      ],
      "source": [
        "run_model('pointwise', no_epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT-dXEGBWBCa"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we instantiated recommendation pipelines in the simplest possible way. Specifically, we have setup the working environment in GDrive, loaded and understood the Movielens 1M dataset, split data in training and test sets, defined a pointwise / pairwise / random / mostpop recommendation algorithm, trained a recommendation model (only for point-wise and pair-wise), computed the user-item matrix that includes the predicted relevance scores, calculated evaluation metrics to monitor properties, and run the full pipeline for the other algorithms under consideration.  \n",
        "\n",
        "## Further Steps\n",
        "\n",
        "- Take a look at the helpers/train_test_splitter.py file and how the existing generators have been defined. \n",
        "- Similarly, take a look at the helpers/instances_creator.py file and how the existing generators have been defined. \n",
        "- A new subclass of the Model class in models/model.py could be defined, implementing a 'train' and a 'predict' method. \n",
        "- The 'test' and 'show_metrics' methods of models/model.py could be extended with the computation needed by a new metric. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}